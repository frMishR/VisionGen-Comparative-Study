{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69983418-884e-4b65-b3c6-f39b5388cb4d",
   "metadata": {},
   "source": [
    "# VisionGen-Comparative-Study (Part 1)\n",
    "\n",
    "_A Comparative Analysis of CGANs and Diffusion Models for Conditional Image Synthesis_\n",
    "\n",
    "---\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Generative models are transforming the field of computer vision, enabling AI systems to generate realistic images from simple inputs like class labels or text prompts. This project investigates:\n",
    "- How do CGANs and Diffusion Models perform on classic and challenging datasets?\n",
    "- What are the differences in sample quality, training stability, and controllability?\n",
    "- Which model is more suitable for high-fidelity, class-conditional image synthesis?\n",
    "\n",
    "## Approach\n",
    "\n",
    "- **CGANs:**  \n",
    "  Implemented and trained on MNIST (digits), Oxford-102 Flowers, and CUB-200 Birds datasets, using fully connected and convolutional architectures.\n",
    "\n",
    "- **Diffusion Models:**  \n",
    "  Integrated modern diffusion pipelines (e.g., DDPM, Stable Diffusion) using open-source libraries. Evaluated on the same datasets for direct comparison.\n",
    "\n",
    "- **Comparative Analysis:**  \n",
    "  Results evaluated both quantitatively (FID, Inception Score) and qualitatively (side-by-side image grids, training dynamics).\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- End-to-end PyTorch and HuggingFace-based implementation\n",
    "- Clean code, modular design, and ready-to-run Jupyter notebooks\n",
    "- Direct, apples-to-apples comparison of CGANs and Diffusion Models\n",
    "- Sample outputs, evaluation metrics, and visualizations included\n",
    "\n",
    "---\n",
    "\n",
    "## Author\n",
    "\n",
    "**Ayushman Mishra**  \n",
    "[GitHub: frMishR](https://github.com/frMishR)\n",
    "\n",
    "---\n",
    "\n",
    "## Research Inspiration\n",
    "\n",
    "This project draws inspiration from foundational work in generative modeling, especially:\n",
    "\n",
    "- **Conditional Generative Adversarial Nets (Mirza & Osindero, 2014):**  \n",
    "  [arXiv:1411.1784](https://arxiv.org/abs/1411.1784)  \n",
    "  The original CGAN paper provided the conceptual and technical basis for the CGAN implementations.\n",
    "\n",
    "Recent advances in diffusion models and open-source research also guided the design of experiments and code.\n",
    "\n",
    "---\n",
    "\n",
    "## Project Origin\n",
    "\n",
    "This project originated as a group submission for:\n",
    "\n",
    "- **Course:** EEE 598: Generative AI – Theory and Practice  \n",
    "- **Professor:** Dr. Lalitha Sankar (Arizona State University)  \n",
    "- **Semester:** Spring 2025\n",
    "\n",
    "### Original Contributors\n",
    "\n",
    "- **Ayushman Mishra** (Solo Upgrade & Comparative Study, 2025)\n",
    "- **Snavya Sai Munti Mudugu Badri Prasad** [GitHub: snavya0309](https://github.com/snavya0309)\n",
    "- **Sushma Niresh** [GitHub: SushmaNiresh](https://github.com/SushmaNiresh)\n",
    "\n",
    "> *Note: This repository represents a major solo upgrade and extension by Ayushman Mishra. All diffusion modeling, comparative analysis, and new documentation were developed independently by Ayushman Mishra (July–September 2025). The original CGAN implementation and dataset preparation were developed collaboratively by the above group.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efca250d-3fd0-4927-a7b7-949fe0c85dcd",
   "metadata": {},
   "source": [
    "# VisionGen – Conditional DCGAN on Multi-Class Datasets\n",
    "\n",
    "> **Notebook Focus**: This notebook implements and trains a **Conditional DCGAN (cDCGAN)** — a class-conditional variant of the Deep Convolutional GAN architecture — on curated image datasets such as **MNIST**, **Oxford 102 Flowers**, and **CUB-200 Birds**.\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "This notebook explores **conditional image generation** using GANs. Unlike standard GANs that generate images without control, **Conditional GANs (cGANs)** allow us to **guide the generation process using class labels** (e.g., “rose,” “eagle,” “digit 3”).\n",
    "\n",
    "We specifically adopt a **DCGAN-style architecture**, integrating label conditioning into both:\n",
    "- The **Generator**, which takes noise + label as input and generates class-specific images\n",
    "- The **Discriminator**, which receives both an image + label and learns to classify whether the image is real or fake **for that label**\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts Covered\n",
    "\n",
    "- **Label conditioning**: Injecting class information into both G and D using one-hot embeddings\n",
    "- **DCGAN Architecture**: Convolutional generator and discriminator for stable image synthesis\n",
    "- **Multi-class Datasets**: Trained on MNIST (digits), Oxford Flowers (102 classes), and CUB Birds (200 fine-grained classes)\n",
    "- **Loss Functions**: Binary Cross Entropy (BCE) for adversarial loss\n",
    "- **Visualization**: Generate and compare outputs for specific class labels\n",
    "- **Training Dynamics**: Track loss curves, sample quality, and class-conditional accuracy visually\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "1. **Imports & Setup** – Libraries, device config, seeds\n",
    "2. **Dataset Handling** – Class-wise folder structure, transformations, label encoding\n",
    "3. **Conditional DCGAN Architecture**\n",
    "   - `Generator`: Accepts noise + class embedding → image\n",
    "   - `Discriminator`: Accepts image + class embedding → real/fake score\n",
    "4. **Training Loop** – Adversarial training with label guidance\n",
    "5. **Sample Generation** – Periodic image sampling for visual inspection\n",
    "6. **(Optional/Planned)**: FID / Inception Score metrics, Diffusion model comparison\n",
    "\n",
    "---\n",
    "\n",
    "## Why this matters\n",
    "\n",
    "This notebook demonstrates:\n",
    "- A full **end-to-end GAN training pipeline** with class-conditioning\n",
    "- How to **scale to large multi-class datasets**\n",
    "- The **visual power of generative models** in robotics, vision, and simulation use-cases\n",
    "\n",
    "It sets the foundation for the upcoming **Diffusion model extension**, which will be added to the same notebook for comparative study.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed393bf-260f-4dc9-a04a-e847f8d41be4",
   "metadata": {},
   "source": [
    "### Setting up GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fbed98-2f8c-47a5-9151-88b2e370d8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(\"Device:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08575ac1-6a85-4426-a6d2-a1da4da56219",
   "metadata": {},
   "source": [
    "### Imports & Setup.\n",
    "\n",
    "- **Python**: 3.11.8\n",
    "- **Key libraries detected**: PIL, difflib, math, matplotlib, numpy, os, random, scipy, sklearn, tarfile, torch, torchmetrics, torchvision, tqdm, urllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d483c4-5230-405d-a670-93b799abca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision numpy matplotlib pillow tqdm scikit-learn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260dfb79-9d46-4918-90d6-c3f13084a2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, random, json, numpy as np, matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "# reproducibility\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cebe550-256a-4dc8-9feb-ef32687cfbb3",
   "metadata": {},
   "source": [
    "## 1. Flower Dataset (Oxford-102)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6706f973-11bb-424f-a698-79c68afcf7a0",
   "metadata": {},
   "source": [
    "### Oxford-102 Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b9f8e8-49af-4afc-b8dd-483f066ac21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Oxford102\"\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size  = 64            # Flowers are heavier than MNIST, so batch smaller\n",
    "z_dim       = 100           # Latent vector dimension\n",
    "y_dim       = 102           # 102 flower categories\n",
    "img_size    = 64            # Resize all to 64x64\n",
    "img_channels= 3             # RGB images\n",
    "img_dim     = img_size * img_size * img_channels\n",
    "lr          = 0.0002\n",
    "epochs      = 200           # Same as MNIST\n",
    "\n",
    "save_dir = f\"./outputs/{dataset_name}\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8752af1-7210-44af-8b9a-2fc522352b2d",
   "metadata": {},
   "source": [
    "### Oxford-102 Dataset Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a6d8af-2726-4c23-a8b6-1ad84f5dd59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import scipy.io\n",
    "\n",
    "# Create target directory\n",
    "oxford102_root = \"./data/Oxford102\"\n",
    "os.makedirs(oxford102_root, exist_ok=True)\n",
    "\n",
    "images_url = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/102flowers.tgz\"\n",
    "labels_url = \"https://www.robots.ox.ac.uk/~vgg/data/flowers/102/imagelabels.mat\"\n",
    "\n",
    "# Target paths\n",
    "images_tar_path = os.path.join(oxford102_root, \"102flowers.tgz\")\n",
    "labels_mat_path = os.path.join(oxford102_root, \"imagelabels.mat\")\n",
    "\n",
    "# Download images archive if not already downloaded\n",
    "if not os.path.exists(images_tar_path):\n",
    "    print(\"Downloading 102flowers.tgz...\")\n",
    "    urllib.request.urlretrieve(images_url, images_tar_path)\n",
    "    print(\"Downloaded images archive!\")\n",
    "\n",
    "# Extract images if not already extracted\n",
    "jpg_folder = os.path.join(oxford102_root, \"jpg\")\n",
    "if not os.path.exists(jpg_folder):\n",
    "    print(\"Extracting images...\")\n",
    "    with tarfile.open(images_tar_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=oxford102_root)\n",
    "    print(\"Extraction complete!\")\n",
    "\n",
    "# Download labels if not already downloaded\n",
    "if not os.path.exists(labels_mat_path):\n",
    "    print(\"Downloading imagelabels.mat...\")\n",
    "    urllib.request.urlretrieve(labels_url, labels_mat_path)\n",
    "    print(\"Downloaded labels file!\")\n",
    "\n",
    "print(\"Oxford-102 dataset downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fa44c8-1aea-4b63-b4b5-23c70c6e2fea",
   "metadata": {},
   "source": [
    "### Oxford-102 Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fd3ca2-8c79-442d-84b7-16a5edc1fb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import scipy.io\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Paths\n",
    "oxford102_root = \"./data/Oxford102\"\n",
    "image_dir = os.path.join(oxford102_root, \"jpg\")\n",
    "labels_mat_path = os.path.join(oxford102_root, \"imagelabels.mat\")\n",
    "\n",
    "# Custom Dataset Class\n",
    "class Oxford102Dataset(Dataset):\n",
    "    def __init__(self, image_dir, labels_mat_path, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Load labels\n",
    "        labels_data = scipy.io.loadmat(labels_mat_path)\n",
    "        self.labels = labels_data[\"labels\"][0]  # (8189,)\n",
    "        \n",
    "        # Load image file names\n",
    "        self.image_files = sorted(os.listdir(image_dir))\n",
    "        \n",
    "        assert len(self.labels) == len(self.image_files), \"Mismatch between number of labels and images!\"\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.image_dir, self.image_files[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[idx] - 1  # MATLAB indexing starts from 1 and not 0\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "# Instantiate dataset and dataloader\n",
    "oxford_dataset = Oxford102Dataset(\n",
    "    image_dir=image_dir,\n",
    "    labels_mat_path=labels_mat_path,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "oxford_loader = DataLoader(\n",
    "    oxford_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "print(f\"Oxford-102 dataset ready: {len(oxford_dataset)} images across {y_dim} classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea455b5f-ec7a-476e-833f-b229328c95cb",
   "metadata": {},
   "source": [
    "### The D & G Setup (Generator64 and Discriminator64) for Oxford-102\n",
    "###### (Concept stays loyal to Original Mirza's cGAN, just upgraded Convolutional layers as images are 64×64×3, not 28×28×1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bef616-935f-487a-9501-ddd96e53a15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "# Utility function to expand one-hot labels spatially\n",
    "def expand_y(y, h, w):\n",
    "    \"\"\"Expand y: (B,C) -> (B,C,h,w) by spatial tiling.\"\"\"\n",
    "    return y.view(y.size(0), y.size(1), 1, 1).expand(-1, -1, h, w)\n",
    "\n",
    "# Generator for 64x64 images\n",
    "class Generator64(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(z_dim + y_dim, 4 * 4 * 512),\n",
    "            nn.BatchNorm1d(4 * 4 * 512),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.conv_blocks = nn.ModuleList([\n",
    "            nn.ConvTranspose2d(512 + y_dim, 256, 4, 2, 1, bias=False),  # 8x8\n",
    "            nn.ConvTranspose2d(256 + y_dim, 128, 4, 2, 1, bias=False),  # 16x16\n",
    "            nn.ConvTranspose2d(128 + y_dim, 64, 4, 2, 1, bias=False),   # 32x32\n",
    "            nn.ConvTranspose2d(64 + y_dim, 3, 4, 2, 1, bias=False)      # 64x64\n",
    "        ])\n",
    "        self.bns = nn.ModuleList([\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.BatchNorm2d(64)\n",
    "        ])\n",
    "\n",
    "    def forward(self, z, y):  # z: (B, 100), y: (B, 102)\n",
    "        x = torch.cat([z, y], dim=1)  # (B, 202)\n",
    "        x = self.fc(x).view(-1, 512, 4, 4)  # (B, 512, 4, 4)\n",
    "\n",
    "        for i, conv in enumerate(self.conv_blocks):\n",
    "            h, w = x.shape[2], x.shape[3]\n",
    "            y_exp = expand_y(y, h, w)\n",
    "            x = torch.cat([x, y_exp], dim=1)\n",
    "            x = conv(x)\n",
    "            if i < 3:\n",
    "                x = self.bns[i](x)\n",
    "                x = nn.ReLU(True)(x)\n",
    "            else:\n",
    "                x = torch.tanh(x)\n",
    "        return x\n",
    "\n",
    "# Discriminator for 64x64 images\n",
    "class Discriminator64(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_blocks = nn.ModuleList([\n",
    "            nn.Conv2d(3 + y_dim, 64, 4, 2, 1, bias=True),   # 32x32\n",
    "            nn.Conv2d(64 + y_dim, 128, 4, 2, 1, bias=True), # 16x16\n",
    "            nn.Conv2d(128 + y_dim, 256, 4, 2, 1, bias=True),# 8x8\n",
    "            nn.Conv2d(256 + y_dim, 512, 4, 2, 1, bias=True) # 4x4\n",
    "        ])\n",
    "        self.fc = nn.Linear(4 * 4 * 512, 1)  # Final linear layer for logits\n",
    "\n",
    "    def forward(self, x, y):  # x: (B, 3, 64, 64), y: (B, 102)\n",
    "        for conv in self.conv_blocks:\n",
    "            h, w = x.shape[2], x.shape[3]\n",
    "            y_exp = expand_y(y, h, w)\n",
    "            x = torch.cat([x, y_exp], dim=1)\n",
    "            x = conv(x)\n",
    "            x = nn.LeakyReLU(0.2, inplace=True)(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)             # Logits (use BCEWithLogitsLoss later)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a8b2e5-2759-4a40-96c2-fd42bcca5952",
   "metadata": {},
   "source": [
    "### Optimizers + Loss Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93dfaef1-0768-42d8-8849-6e4e04eb89d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "\n",
    "# Initialize models\n",
    "G = Generator64().to(DEVICE)\n",
    "D = Discriminator64().to(DEVICE)\n",
    "\n",
    "# Optimizers\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "# Loss Function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(\"Optimizers and Loss function = READY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0bd64e3-b940-466a-9ab2-2c37f8e57d67",
   "metadata": {},
   "source": [
    "### One-Hot Encoding Helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b891b0d-9879-40f8-9c26-874ffdd807e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_hot(labels, num_classes):\n",
    "    \"\"\"\n",
    "    Converts integer labels into one-hot encoded vectors.\n",
    "    labels: (batch_size,) --> returns (batch_size, num_classes)\n",
    "    \"\"\"\n",
    "    return torch.zeros(labels.size(0), num_classes, device=labels.device).scatter_(1, labels.unsqueeze(1), 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0122a80-810e-41ff-8cdf-af6620518886",
   "metadata": {},
   "source": [
    "### Save Image Grid (Oxford-102, RGB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f815c6-8e5f-480e-9837-295b68e481d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def save_image_grid(images, labels, epoch, save_dir, nrow=8):\n",
    "    \"\"\"\n",
    "    Save a grid of generated images grouped by their labels.\n",
    "    \"\"\"\n",
    "    images = images.detach().cpu()\n",
    "    images = (images + 1) / 2  # Denormalize from [-1,1] to [0,1]\n",
    "    \n",
    "    batch_size = images.size(0)\n",
    "    fig, axes = plt.subplots(nrows=(batch_size // nrow) + 1, ncols=nrow, figsize=(nrow * 2, (batch_size // nrow) * 2))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for img, ax in zip(images, axes):\n",
    "        img = img.permute(1, 2, 0)  # (C,H,W) --> (H,W,C)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    for ax in axes[batch_size:]:\n",
    "        ax.axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(save_dir, f\"epoch_{epoch:03d}.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7361830a-bed1-4bcc-809e-21e7bb599f51",
   "metadata": {},
   "source": [
    "## Training Loop for Oxford-102 (Please 'DO NOT RUN' unless willing to RE-TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8c78936-054e-4b6c-9311-e0d6745cdc7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fixed noise and labels for consistent evaluation\n",
    "fixed_z = torch.randn(y_dim * 2, z_dim, device=DEVICE)\n",
    "fixed_labels = torch.arange(y_dim, device=DEVICE).repeat_interleave(2)\n",
    "fixed_labels_onehot = make_one_hot(fixed_labels, y_dim)\n",
    "\n",
    "D_losses = []\n",
    "G_losses = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loop = tqdm(oxford_loader, leave=False, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "    for real_imgs, labels in loop:\n",
    "        real_imgs, labels = real_imgs.to(DEVICE), labels.to(DEVICE)\n",
    "        bsize = real_imgs.size(0)\n",
    "        \n",
    "        real_targets = torch.ones(bsize, 1, device=DEVICE)\n",
    "        fake_targets = torch.zeros(bsize, 1, device=DEVICE)\n",
    "        \n",
    "        labels_onehot = make_one_hot(labels.long(), y_dim)  \n",
    "\n",
    "        # Train Discriminator\n",
    "        D_optimizer.zero_grad()\n",
    "        \n",
    "        D_real = D(real_imgs, labels_onehot)\n",
    "        D_loss_real = criterion(D_real, real_targets)\n",
    "        \n",
    "        z = torch.randn(bsize, z_dim, device=DEVICE)\n",
    "        fake_imgs = G(z, labels_onehot)\n",
    "        D_fake = D(fake_imgs.detach(), labels_onehot)\n",
    "        D_loss_fake = criterion(D_fake, fake_targets)\n",
    "        \n",
    "        D_loss = D_loss_real + D_loss_fake\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "        \n",
    "        # Train Generator\n",
    "        G_optimizer.zero_grad()\n",
    "        \n",
    "        z = torch.randn(bsize, z_dim, device=DEVICE)\n",
    "        fake_imgs = G(z, labels_onehot)\n",
    "        D_fake = D(fake_imgs, labels_onehot)\n",
    "        G_loss = criterion(D_fake, real_targets)\n",
    "        \n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "        \n",
    "        D_losses.append(D_loss.item())\n",
    "        G_losses.append(G_loss.item())\n",
    "        \n",
    "    \n",
    "    print(f\"Epoch [{epoch}/{epochs}]  D_loss: {D_loss.item():.4f}  G_loss: {G_loss.item():.4f}\")\n",
    "    \n",
    "    # Save and Show Fake Samples every 10 epochs\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        with torch.no_grad():\n",
    "            fake_imgs_fixed = G(fixed_z, fixed_labels_onehot)\n",
    "        \n",
    "        # Save to file\n",
    "        save_image_grid(fake_imgs_fixed, fixed_labels, epoch, save_dir)\n",
    "        \n",
    "        # Show in notebook\n",
    "        fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n",
    "        axes = axes.flatten()\n",
    "        fake_imgs_fixed = fake_imgs_fixed.detach().cpu()\n",
    "        fake_imgs_fixed = (fake_imgs_fixed + 1) / 2  # denormalize\n",
    "\n",
    "        for img, ax in zip(fake_imgs_fixed, axes):\n",
    "            img = img.permute(1, 2, 0)\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "\n",
    "        for ax in axes[len(fake_imgs_fixed):]:\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.suptitle(f\"Generated Samples at Epoch {epoch}\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "np.save(os.path.join(save_dir, \"D_losses.npy\"), np.array(D_losses))\n",
    "np.save(os.path.join(save_dir, \"G_losses.npy\"), np.array(G_losses))\n",
    "torch.save(G.state_dict(), os.path.join(save_dir, \"generator.pth\"))\n",
    "torch.save(D.state_dict(), os.path.join(save_dir, \"discriminator.pth\"))\n",
    "\n",
    "print(\"Training done and all models and losses saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e986a44-52ec-4a05-9409-d74164b8bc7e",
   "metadata": {},
   "source": [
    "### Load Trained Generator + Generate Flowers + Map Label to Flower Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59925a5-9399-4887-b642-65b930d5daf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Load trained Generator\n",
    "G = Generator64().to(DEVICE)\n",
    "G.load_state_dict(torch.load(os.path.join(save_dir, \"generator.pth\"), map_location=DEVICE))\n",
    "G.eval()  # Set to evaluation mode\n",
    "\n",
    "# Prepare Correct Class Names (Oxford-102 flowers)\n",
    "class_names = [\n",
    "    'pink primrose', 'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea', 'english marigold',\n",
    "    'tiger lily', 'moon orchid', 'bird of paradise', 'monkshood', 'globe thistle', 'snapdragon',\n",
    "    \"colt's foot\", 'king protea', 'spear thistle', 'yellow iris', 'globe-flower', 'purple coneflower',\n",
    "    'peruvian lily', 'balloon flower', 'giant white arum lily', 'fire lily', 'pincushion flower', 'fritillary',\n",
    "    'red ginger', 'grape hyacinth', 'corn poppy', 'prince of wales feathers', 'stemless gentian', 'artichoke',\n",
    "    'sweet william', 'carnation', 'garden phlox', 'love in the mist', 'mexican aster', 'alpine sea holly',\n",
    "    'ruby-lipped cattleya', 'cape flower', 'great masterwort', 'siam tulip', 'lenten rose', 'barbeton daisy',\n",
    "    'daffodil', 'sword lily', 'poinsettia', 'bolero deep blue', 'wallflower', 'marigold', 'buttercup',\n",
    "    'oxeye daisy', 'common dandelion', 'petunia', 'wild pansy', 'primula', 'sunflower', 'pelargonium',\n",
    "    'bishop of llandaff', 'gaura', 'geranium', 'orange dahlia', 'pink-yellow dahlia?', 'cautleya spicata',\n",
    "    'japanese anemone', 'black-eyed susan', 'silverbush', 'californian poppy', 'osteospermum', 'spring crocus',\n",
    "    'bearded iris', 'windflower', 'tree poppy', 'gazania', 'azalea', 'water lily', 'rose', 'thorn apple',\n",
    "    'morning glory', 'passion flower', 'lotus', 'toad lily', 'anthurium', 'frangipani', 'clematis', 'hibiscus',\n",
    "    'columbine', 'desert-rose', 'tree mallow', 'magnolia', 'cyclamen ', 'watercress', 'canna lily', 'hippeastrum ',\n",
    "    'bee balm', 'ball moss', 'foxglove', 'bougainvillea', 'camellia', 'mallow', 'mexican petunia', 'bromelia',\n",
    "    'blanket flower', 'trumpet creeper', 'blackberry lily'\n",
    "]\n",
    "\n",
    "# Generate samples\n",
    "fixed_z = torch.randn(y_dim, z_dim, device=DEVICE)  # 1 sample per class\n",
    "fixed_labels = torch.arange(y_dim, device=DEVICE)\n",
    "fixed_labels_onehot = make_one_hot(fixed_labels, y_dim)\n",
    "\n",
    "with torch.no_grad():\n",
    "    fake_imgs = G(fixed_z, fixed_labels_onehot)\n",
    "\n",
    "# Show and map flowers\n",
    "fig, axes = plt.subplots(8, 13, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "fake_imgs = (fake_imgs + 1) / 2  # Denormalize from [-1,1] to [0,1]\n",
    "\n",
    "for idx, (img, ax) in enumerate(zip(fake_imgs, axes)):\n",
    "    img = img.detach().cpu().permute(1, 2, 0)\n",
    "    ax.imshow(img)\n",
    "    if idx < len(class_names):\n",
    "        ax.set_title(f\"{class_names[idx]}\", fontsize=6)\n",
    "    ax.axis('off')\n",
    "\n",
    "# Hide extra axes if any\n",
    "for ax in axes[len(fake_imgs):]:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f38aa1-3229-4a7b-9a60-f29414d66d8d",
   "metadata": {},
   "source": [
    "### Markdown list of 102 Flower Classes (Oxford-102)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ab774c-2a5e-488c-bf18-e707a4ce87bc",
   "metadata": {},
   "source": [
    "1. Pink primrose\n",
    "2. Hard-leaved pocket orchid\n",
    "3. Canterbury bells\n",
    "4. Sweet pea\n",
    "5. English marigold\n",
    "6. Tiger lily\n",
    "7. Moon orchid\n",
    "8. Bird of paradise\n",
    "9. Monkshood\n",
    "10. Globe thistle\n",
    "11. Snapdragon\n",
    "12. Colt’s foot\n",
    "13. King protea\n",
    "14. Spear thistle\n",
    "15. Yellow iris\n",
    "16. Globe-flower\n",
    "17. Purple coneflower\n",
    "18. Peruvian lily\n",
    "19. Ball moss\n",
    "20. Mexican petunia\n",
    "21. Bromelia\n",
    "22. Blanket flower\n",
    "23. Trumpet creeper\n",
    "24. Black-eyed susan\n",
    "25. Pontederia\n",
    "26. Bolero deep blue\n",
    "27. Bougainvillea\n",
    "28. Camellia\n",
    "29. Mallow\n",
    "30. Mexican aster\n",
    "31. Alpine sea holly\n",
    "32. Ruby-lipped cattleya\n",
    "33. Cape flower\n",
    "34. Great masterwort\n",
    "35. Siam tulip\n",
    "36. Lenten rose\n",
    "37. Barbeton daisy\n",
    "38. Daffodil\n",
    "39. Sword lily\n",
    "40. Poinsettia\n",
    "41. Gaura\n",
    "42. Geranium\n",
    "43. Orange dahlia\n",
    "44. Pink-yellow dahlia\n",
    "45. Cautleya spicata\n",
    "46. Japanese anemone\n",
    "47. Blackberry lily\n",
    "48. Tree poppy\n",
    "49. Gazania\n",
    "50. Azalea\n",
    "51. Water lily\n",
    "52. Rose\n",
    "53. Thorn apple\n",
    "54. Morning glory\n",
    "55. Passion flower\n",
    "56. Lotus\n",
    "57. Toad lily\n",
    "58. Anthurium\n",
    "59. Frangipani\n",
    "60. Clematis\n",
    "61. Hibiscus\n",
    "62. Columbine\n",
    "63. Desert-rose\n",
    "64. Tree mallow\n",
    "65. Magnolia\n",
    "66. Cyclamen\n",
    "67. Watercress\n",
    "68. Canna lily\n",
    "69. Hippeastrum\n",
    "70. Bee balm\n",
    "71. Balloon flower\n",
    "72. Oxeye daisy\n",
    "73. Fire lily\n",
    "74. Pincushion flower\n",
    "75. Fritillary\n",
    "76. Red ginger\n",
    "77. Grape hyacinth\n",
    "78. Corn poppy\n",
    "79. Prince of wales feathers\n",
    "80. Stemless gentian\n",
    "81. Artichoke\n",
    "82. Sweet william\n",
    "83. Carnation\n",
    "84. Garden phlox\n",
    "85. Love in the mist\n",
    "86. Mexican sunflower\n",
    "87. Wild pansy\n",
    "88. Primula\n",
    "89. Sunflower\n",
    "90. Pelargonium\n",
    "91. Bishop of llandaff\n",
    "92. Gaillardia\n",
    "93. Buttercup\n",
    "94. Oxlip\n",
    "95. Tiger flower\n",
    "96. Rose mallow\n",
    "97. Snapdragon\n",
    "98. Columbine\n",
    "99. Colt’s foot\n",
    "100. King protea\n",
    "101. Spear thistle\n",
    "102. Globe thistle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3c2856-2673-4e1c-9a75-eb1e7994239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "\n",
    "# Paths\n",
    "real_image_folder = os.path.join(oxford102_root, \"jpg\")\n",
    "labels_mat_path = os.path.join(oxford102_root, \"imagelabels.mat\")\n",
    "\n",
    "class_names = [\n",
    "    'pink primrose', 'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea', 'english marigold',\n",
    "    'tiger lily', 'moon orchid', 'bird of paradise', 'monkshood', 'globe thistle', 'snapdragon',\n",
    "    \"colt's foot\", 'king protea', 'spear thistle', 'yellow iris', 'globe-flower', 'purple coneflower',\n",
    "    'peruvian lily', 'balloon flower', 'giant white arum lily', 'fire lily', 'pincushion flower', 'fritillary',\n",
    "    'red ginger', 'grape hyacinth', 'corn poppy', 'prince of wales feathers', 'stemless gentian', 'artichoke',\n",
    "    'sweet william', 'carnation', 'garden phlox', 'love in the mist', 'mexican aster', 'alpine sea holly',\n",
    "    'ruby-lipped cattleya', 'cape flower', 'great masterwort', 'siam tulip', 'lenten rose', 'barbeton daisy',\n",
    "    'daffodil', 'sword lily', 'poinsettia', 'bolero deep blue', 'wallflower', 'marigold', 'buttercup',\n",
    "    'oxeye daisy', 'common dandelion', 'petunia', 'wild pansy', 'primula', 'sunflower', 'pelargonium',\n",
    "    'bishop of llandaff', 'gaura', 'geranium', 'orange dahlia', 'pink-yellow dahlia?', 'cautleya spicata',\n",
    "    'japanese anemone', 'black-eyed susan', 'silverbush', 'californian poppy', 'osteospermum', 'spring crocus',\n",
    "    'bearded iris', 'windflower', 'tree poppy', 'gazania', 'azalea', 'water lily', 'rose', 'thorn apple',\n",
    "    'morning glory', 'passion flower', 'lotus', 'toad lily', 'anthurium', 'frangipani', 'clematis', 'hibiscus',\n",
    "    'columbine', 'desert-rose', 'tree mallow', 'magnolia', 'cyclamen ', 'watercress', 'canna lily', 'hippeastrum ',\n",
    "    'bee balm', 'ball moss', 'foxglove', 'bougainvillea', 'camellia', 'mallow', 'mexican petunia', 'bromelia',\n",
    "    'blanket flower', 'trumpet creeper', 'blackberry lily'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2f95c58-ca1e-4c52-ba8b-a050532c694d",
   "metadata": {},
   "source": [
    "#### Example : Corn Poppy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a31e22-9fb2-4e83-ae6b-656b06bdb99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Flower\n",
    "selected_flower_name = \"corn poppy\"\n",
    "selected_label = class_names.index(selected_flower_name)\n",
    "\n",
    "print(f\"Selected Flower: {selected_flower_name} (Label {selected_label})\")\n",
    "\n",
    "# Load labels\n",
    "labels_data = scipy.io.loadmat(labels_mat_path)\n",
    "real_labels = labels_data[\"labels\"][0] - 1  # MATLAB .mat files are 1-indexed and not zero.\n",
    "\n",
    "# Find all images matching selected label\n",
    "image_files = sorted(os.listdir(real_image_folder))\n",
    "matching_indices = [i for i, lbl in enumerate(real_labels) if lbl == selected_label]\n",
    "\n",
    "# Randomly pick one real sample\n",
    "real_idx = random.choice(matching_indices)\n",
    "real_img_path = os.path.join(real_image_folder, image_files[real_idx])\n",
    "\n",
    "# Load real image\n",
    "real_img = Image.open(real_img_path).convert(\"RGB\")\n",
    "real_img = real_img.resize((img_size, img_size))\n",
    "\n",
    "# Generate fake image\n",
    "z = torch.randn(1, z_dim, device=DEVICE)\n",
    "label_tensor = torch.tensor([selected_label], device=DEVICE)\n",
    "label_onehot = make_one_hot(label_tensor, y_dim)\n",
    "\n",
    "with torch.no_grad():\n",
    "    fake_img = G(z, label_onehot)\n",
    "\n",
    "fake_img = (fake_img + 1) / 2  # Denormalize\n",
    "fake_img = fake_img.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "# side-by-side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# Real image\n",
    "axes[0].imshow(real_img)\n",
    "axes[0].set_title(f\"Real: {selected_flower_name}\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Generated image\n",
    "axes[1].imshow(fake_img)\n",
    "axes[1].set_title(f\"Generated: {selected_flower_name}\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7956624c-e892-4538-97cb-e1ec98056627",
   "metadata": {},
   "source": [
    "#### Example : English Marigold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d271fc-a215-43e5-8754-299ca6aa5b9e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Selecting Flower\n",
    "selected_flower_name = \"english marigold\"\n",
    "selected_label = class_names.index(selected_flower_name)\n",
    "\n",
    "print(f\"Selected Flower: {selected_flower_name} (Label {selected_label})\")\n",
    "\n",
    "# Load labels\n",
    "labels_data = scipy.io.loadmat(labels_mat_path)\n",
    "real_labels = labels_data[\"labels\"][0] - 1  \n",
    "\n",
    "# Find all images matching selected label\n",
    "image_files = sorted(os.listdir(real_image_folder))\n",
    "matching_indices = [i for i, lbl in enumerate(real_labels) if lbl == selected_label]\n",
    "\n",
    "# Randomly pick one real sample\n",
    "real_idx = random.choice(matching_indices)\n",
    "real_img_path = os.path.join(real_image_folder, image_files[real_idx])\n",
    "\n",
    "# Load real image\n",
    "real_img = Image.open(real_img_path).convert(\"RGB\")\n",
    "real_img = real_img.resize((img_size, img_size))\n",
    "\n",
    "# Generate fake image\n",
    "z = torch.randn(1, z_dim, device=DEVICE)\n",
    "label_tensor = torch.tensor([selected_label], device=DEVICE)\n",
    "label_onehot = make_one_hot(label_tensor, y_dim)\n",
    "\n",
    "with torch.no_grad():\n",
    "    fake_img = G(z, label_onehot)\n",
    "\n",
    "fake_img = (fake_img + 1) / 2  # Denormalize\n",
    "fake_img = fake_img.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "# side-by-side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# Real image\n",
    "axes[0].imshow(real_img)\n",
    "axes[0].set_title(f\"Real: {selected_flower_name}\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Generated image\n",
    "axes[1].imshow(fake_img)\n",
    "axes[1].set_title(f\"Generated: {selected_flower_name}\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6cebf1-e05d-40e2-950a-58616d8f38c0",
   "metadata": {},
   "source": [
    "#### Example : Snapdragon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aae3250-a3ea-43cd-9e75-85fad64b5495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting Flower\n",
    "selected_flower_name = \"snapdragon\"\n",
    "selected_label = class_names.index(selected_flower_name)\n",
    "\n",
    "print(f\"Selected Flower: {selected_flower_name} (Label {selected_label})\")\n",
    "\n",
    "# Load labels\n",
    "labels_data = scipy.io.loadmat(labels_mat_path)\n",
    "real_labels = labels_data[\"labels\"][0] - 1  \n",
    "\n",
    "# Find all images matching selected label\n",
    "image_files = sorted(os.listdir(real_image_folder))\n",
    "matching_indices = [i for i, lbl in enumerate(real_labels) if lbl == selected_label]\n",
    "\n",
    "# Randomly pick one real sample\n",
    "real_idx = random.choice(matching_indices)\n",
    "real_img_path = os.path.join(real_image_folder, image_files[real_idx])\n",
    "\n",
    "# Load real image\n",
    "real_img = Image.open(real_img_path).convert(\"RGB\")\n",
    "real_img = real_img.resize((img_size, img_size))\n",
    "\n",
    "# Generate fake image\n",
    "z = torch.randn(1, z_dim, device=DEVICE)\n",
    "label_tensor = torch.tensor([selected_label], device=DEVICE)\n",
    "label_onehot = make_one_hot(label_tensor, y_dim)\n",
    "\n",
    "with torch.no_grad():\n",
    "    fake_img = G(z, label_onehot)\n",
    "\n",
    "fake_img = (fake_img + 1) / 2  # Denormalize\n",
    "fake_img = fake_img.squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "# side-by-side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# Real image\n",
    "axes[0].imshow(real_img)\n",
    "axes[0].set_title(f\"Real: {selected_flower_name}\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Generated image\n",
    "axes[1].imshow(fake_img)\n",
    "axes[1].set_title(f\"Generated: {selected_flower_name}\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168dce1b-dc13-4515-9e56-01f008e4f031",
   "metadata": {},
   "source": [
    "### FID / IS for Oxford-102 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64690206-9e23-4efd-818a-177a553027af",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch-fidelity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9750cdc-f261-44a1-bcbd-79dbc646f9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.image.fid import FrechetInceptionDistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ecb5b69-3166-4c36-a98c-2673f782ee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "import numpy as np\n",
    "\n",
    "# number of samples to evaluate\n",
    "num_samples = 1000\n",
    "batch_eval  = 64  \n",
    "\n",
    "# Generate all fake images in CPU-friendly batches\n",
    "fake_uint8_list = []\n",
    "G.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(0, num_samples, batch_eval):\n",
    "        z_batch = torch.randn(batch_eval, z_dim, device=DEVICE)\n",
    "        labels = torch.randint(0, y_dim, (batch_eval,), device=DEVICE)\n",
    "        oh = make_one_hot(labels, y_dim)\n",
    "        imgs = G(z_batch, oh).cpu()                 # (B,3,64,64) float in [-1,1]\n",
    "        imgs = (imgs + 1) / 2                       # to [0,1]\n",
    "        imgs = (imgs * 255).clamp(0,255).to(torch.uint8)\n",
    "        fake_uint8_list.append(imgs)\n",
    "fake_uint8 = torch.cat(fake_uint8_list, dim=0)\n",
    "\n",
    "# Sample real images from the dataset in batches\n",
    "real_uint8_list = []\n",
    "real_loader = DataLoader(\n",
    "    oxford_dataset, batch_size=batch_eval, shuffle=True, drop_last=True, num_workers=0\n",
    ")\n",
    "needed = num_samples\n",
    "for real_imgs, _ in real_loader:\n",
    "    if needed <= 0:\n",
    "        break\n",
    "    real = real_imgs[:needed].cpu()              # (B,3,64,64) normalized [-1,1]? no, ours is [–1,1]\n",
    "    real = (real * 0.5 + 0.5)                    # to [0,1]\n",
    "    real = (real * 255).clamp(0,255).to(torch.uint8)\n",
    "    real_uint8_list.append(real)\n",
    "    needed -= real.shape[0]\n",
    "real_uint8 = torch.cat(real_uint8_list, dim=0)\n",
    "\n",
    "# Instantiate metrics (compute_on_cpu to avoid GPU OOM)\n",
    "fid = FrechetInceptionDistance(feature=64, compute_on_cpu=True).to('cpu')\n",
    "is_scorer = InceptionScore(compute_on_cpu=True).to('cpu')\n",
    "\n",
    "# Update FID in small chunks\n",
    "for i in range(0, num_samples, batch_eval):\n",
    "    fid.update(real_uint8[i:i+batch_eval], real=True)\n",
    "    fid.update(fake_uint8[i:i+batch_eval], real=False)\n",
    "fid_score = fid.compute().item()\n",
    "\n",
    "# Update Inception Score on fake images\n",
    "for i in range(0, num_samples, batch_eval):\n",
    "    is_scorer.update(fake_uint8[i:i+batch_eval])\n",
    "is_score, is_std = is_scorer.compute()\n",
    "\n",
    "\n",
    "print(f\"FID (real vs generated): {fid_score:.2f}\")\n",
    "print(f\"Inception Score: {is_score:.2f} ± {is_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7983df-8d52-4034-a83b-d10b065529e1",
   "metadata": {},
   "source": [
    "### Loss Plot for Oxford-102"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db06d0c-46a1-4a2d-a890-ad0570796f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def moving_average(x, window_size=200):\n",
    "    return np.convolve(x, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# Path to saved losses\n",
    "save_dir = \"./outputs/Oxford102\"\n",
    "D_losses = np.load(os.path.join(save_dir, \"D_losses.npy\"))\n",
    "G_losses = np.load(os.path.join(save_dir, \"G_losses.npy\"))\n",
    "\n",
    "# Plot smoothed loss curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(moving_average(D_losses), label=\"Discriminator Loss\", color=\"tab:blue\", alpha=0.9)\n",
    "plt.plot(moving_average(G_losses), label=\"Generator Loss\", color=\"tab:orange\", alpha=0.9)\n",
    "plt.xlabel(\"Training Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Oxford-102 cGAN Training Loss Curves (Smoothed)\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(save_dir, \"oxford102_cgan_loss_curve.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab2bd80-1041-4e54-8d88-172592de89eb",
   "metadata": {},
   "source": [
    "## 2. Bird Dataset (CUB-200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d3ff74-fc8a-48da-b4c6-1f48ae29203f",
   "metadata": {},
   "source": [
    "### Configuration & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc7806e-2b6b-4f82-b65e-013828a78ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "dataset_name = \"CUB200\"\n",
    "\n",
    "# Hyperparameters\n",
    "batch_size  = 64\n",
    "z_dim       = 100\n",
    "y_dim       = 200          # 200 bird species\n",
    "img_size    = 128\n",
    "img_channels= 3\n",
    "img_dim     = img_size * img_size * img_channels\n",
    "lr          = 0.0002\n",
    "epochs      = 200\n",
    "\n",
    "save_dir = f\"./outputs/{dataset_name}\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61646dcf-34b7-4a12-9fee-cefd67ed044e",
   "metadata": {},
   "source": [
    "### Dataset Download and Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9160fe-cac4-427d-8d74-739b77c0498a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "cub_root = \"./data/CUB_200_2011\"\n",
    "cub_tar_path = \"./data/CUB_200_2011.tgz\"\n",
    "cub_url = \"https://data.caltech.edu/records/65de6-vp158/files/CUB_200_2011.tgz\"\n",
    "\n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "\n",
    "# Download dataset if not present\n",
    "if not os.path.exists(cub_tar_path):\n",
    "    print(\"Downloading CUB-200-2011...\")\n",
    "    urllib.request.urlretrieve(cub_url, cub_tar_path)\n",
    "    print(\"Downloaded!\")\n",
    "\n",
    "# Extract dataset if not already extracted\n",
    "if not os.path.exists(cub_root):\n",
    "    print(\"Extracting CUB-200-2011...\")\n",
    "    with tarfile.open(cub_tar_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=\"./data\")\n",
    "    print(\"Extracted!\")\n",
    "\n",
    "print(\"CUB-200 dataset ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89df732-49b4-40fe-ae00-b214f6da1c62",
   "metadata": {},
   "source": [
    "### Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9451e1d-1e41-4799-a422-cafab4c6b704",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define transforms\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
    "])\n",
    "\n",
    "# Custom Dataset\n",
    "class CUBDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Read image paths\n",
    "        with open(os.path.join(root_dir, \"images.txt\")) as f:\n",
    "            self.image_paths = [line.strip().split()[1] for line in f.readlines()]\n",
    "        \n",
    "        # Read labels\n",
    "        with open(os.path.join(root_dir, \"image_class_labels.txt\")) as f:\n",
    "            self.labels = [int(line.strip().split()[1]) - 1 for line in f.readlines()]  # readme indexing\n",
    "\n",
    "        assert len(self.image_paths) == len(self.labels), \"Mismatch between images and labels\"\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root_dir, \"images\", self.image_paths[idx])\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "# Instantiate\n",
    "cub_dataset = CUBDataset(cub_root, transform=transform)\n",
    "cub_loader = DataLoader(cub_dataset, batch_size=batch_size, shuffle=True, drop_last=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "print(f\"CUB-200 Dataset ready: {len(cub_dataset)} images across {y_dim} classes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba38825-1e5f-4a19-a84d-b6c9627aecd8",
   "metadata": {},
   "source": [
    "### Generator128 and Discriminator128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b863165-8f6e-4062-b513-2cb3dd4043db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def expand_y(y, h, w):\n",
    "    return y.view(y.size(0), y.size(1), 1, 1).expand(-1, -1, h, w)\n",
    "\n",
    "class Generator128(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # project z+y → 8×8×512\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(z_dim + y_dim, 8 * 8 * 512),\n",
    "            nn.BatchNorm1d(8 * 8 * 512),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.conv_blocks = nn.ModuleList([\n",
    "            nn.ConvTranspose2d(512 + y_dim, 256, 4, 2, 1, bias=False),  # 8→16\n",
    "            nn.ConvTranspose2d(256 + y_dim, 128, 4, 2, 1, bias=False),  # 16→32\n",
    "            nn.ConvTranspose2d(128 + y_dim,  64, 4, 2, 1, bias=False),  # 32→64\n",
    "            nn.ConvTranspose2d( 64 + y_dim,   3, 4, 2, 1, bias=False),  # 64→128\n",
    "        ])\n",
    "        self.bns = nn.ModuleList([\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.BatchNorm2d(64),\n",
    "        ])\n",
    "\n",
    "    def forward(self, z, y):\n",
    "        # z: (B, z_dim), y: (B, y_dim)\n",
    "        x = torch.cat([z, y], dim=1)\n",
    "        x = self.fc(x).view(-1, 512, 8, 8)\n",
    "        for i, conv in enumerate(self.conv_blocks):\n",
    "            h, w = x.shape[2:]\n",
    "            y_exp = expand_y(y, h, w)\n",
    "            x = torch.cat([x, y_exp], dim=1)\n",
    "            x = conv(x)\n",
    "            if i < len(self.bns):\n",
    "                x = self.bns[i](x)\n",
    "                x = nn.ReLU(True)(x)\n",
    "            else:\n",
    "                x = torch.tanh(x)\n",
    "        return x\n",
    "\n",
    "class Discriminator128(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_blocks = nn.ModuleList([\n",
    "            nn.Conv2d(3 + y_dim,   64, 4, 2, 1, bias=True),  # 128→64\n",
    "            nn.Conv2d(64 + y_dim, 128, 4, 2, 1, bias=True),  # 64→32\n",
    "            nn.Conv2d(128 + y_dim,256, 4, 2, 1, bias=True),  # 32→16\n",
    "            nn.Conv2d(256 + y_dim,512, 4, 2, 1, bias=True),  # 16→8\n",
    "        ])\n",
    "        self.fc = nn.Linear(8 * 8 * 512, 1)\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        # x: (B, 3, 128,128), y: (B, y_dim)\n",
    "        for conv in self.conv_blocks:\n",
    "            h, w = x.shape[2:]\n",
    "            y_exp = expand_y(y, h, w)\n",
    "            x = torch.cat([x, y_exp], dim=1)\n",
    "            x = conv(x)\n",
    "            x = nn.LeakyReLU(0.2, inplace=True)(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3478ed9-39e7-44b2-b831-5510af26e694",
   "metadata": {},
   "source": [
    "### Optimizers and Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e378d64c-ab81-4e26-8bda-67337098b8c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "G = Generator128().to(DEVICE)\n",
    "D = Discriminator128().to(DEVICE)\n",
    "\n",
    "G_optimizer = optim.Adam(G.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "D_optimizer = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.999))\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "print(\"Success!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59d0f76-5a73-4959-9862-cda8164f0c20",
   "metadata": {},
   "source": [
    "### Helper Functions (make_one_hot, save_image_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d5ec7a-1a3b-4ec1-909f-1d6981789d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_hot(labels, num_classes):\n",
    "    return torch.zeros(labels.size(0), num_classes, device=labels.device).scatter_(1, labels.unsqueeze(1), 1)\n",
    "\n",
    "def save_image_grid(images, labels, epoch, save_dir, nrow=8):\n",
    "    import matplotlib.pyplot as plt\n",
    "    images = images.detach().cpu()\n",
    "    images = (images + 1) / 2\n",
    "    batch_size = images.size(0)\n",
    "    fig, axes = plt.subplots(nrows=(batch_size // nrow) + 1, ncols=nrow, figsize=(nrow*2, (batch_size//nrow)*2))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for img, ax in zip(images, axes):\n",
    "        img = img.permute(1, 2, 0)\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "\n",
    "    for ax in axes[batch_size:]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    plt.savefig(os.path.join(save_dir, f\"epoch_{epoch:03d}.png\"))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8749f9a4-ea22-4290-aaf2-efcace8ea03b",
   "metadata": {},
   "source": [
    "## Training Loop for CUB-200 (Please 'DO NOT RUN' unless willing to RE-TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4514014b-ae95-4cde-a886-c61fb635fc04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fixed_z = torch.randn(y_dim * 2, z_dim, device=DEVICE)\n",
    "fixed_labels = torch.arange(y_dim, device=DEVICE).repeat_interleave(2)\n",
    "fixed_labels_onehot = make_one_hot(fixed_labels, y_dim)\n",
    "\n",
    "D_losses = []\n",
    "G_losses = []\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    loop = tqdm(cub_loader, leave=False, desc=f\"Epoch {epoch}/{epochs}\")\n",
    "    for real_imgs, labels in loop:\n",
    "        real_imgs, labels = real_imgs.to(DEVICE), labels.to(DEVICE)\n",
    "        bsize = real_imgs.size(0)\n",
    "\n",
    "        real_targets = torch.ones(bsize, 1, device=DEVICE)\n",
    "        fake_targets = torch.zeros(bsize, 1, device=DEVICE)\n",
    "\n",
    "        labels_onehot = make_one_hot(labels, y_dim)\n",
    "\n",
    "        D_optimizer.zero_grad()\n",
    "        D_real = D(real_imgs, labels_onehot)\n",
    "        D_loss_real = criterion(D_real, real_targets)\n",
    "\n",
    "        z = torch.randn(bsize, z_dim, device=DEVICE)\n",
    "        fake_imgs = G(z, labels_onehot)\n",
    "        D_fake = D(fake_imgs.detach(), labels_onehot)\n",
    "        D_loss_fake = criterion(D_fake, fake_targets)\n",
    "\n",
    "        D_loss = D_loss_real + D_loss_fake\n",
    "        D_loss.backward()\n",
    "        D_optimizer.step()\n",
    "\n",
    "        G_optimizer.zero_grad()\n",
    "        z = torch.randn(bsize, z_dim, device=DEVICE)\n",
    "        fake_imgs = G(z, labels_onehot)\n",
    "        D_fake = D(fake_imgs, labels_onehot)\n",
    "        G_loss = criterion(D_fake, real_targets)\n",
    "\n",
    "        G_loss.backward()\n",
    "        G_optimizer.step()\n",
    "\n",
    "        D_losses.append(D_loss.item())\n",
    "        G_losses.append(G_loss.item())\n",
    "\n",
    "    print(f\"Epoch [{epoch}/{epochs}]  D_loss: {D_loss.item():.4f}  G_loss: {G_loss.item():.4f}\")\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        with torch.no_grad():\n",
    "            fake_imgs_fixed = G(fixed_z, fixed_labels_onehot)\n",
    "\n",
    "        save_image_grid(fake_imgs_fixed, fixed_labels, epoch, save_dir)\n",
    "\n",
    "        fig, axes = plt.subplots(4, 8, figsize=(16, 8))\n",
    "        axes = axes.flatten()\n",
    "        fake_imgs_fixed = fake_imgs_fixed.detach().cpu()\n",
    "        fake_imgs_fixed = (fake_imgs_fixed + 1) / 2\n",
    "\n",
    "        for img, ax in zip(fake_imgs_fixed, axes):\n",
    "            img = img.permute(1, 2, 0)\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "\n",
    "        for ax in axes[len(fake_imgs_fixed):]:\n",
    "            ax.axis('off')\n",
    "\n",
    "        plt.suptitle(f\"Generated Samples at Epoch {epoch}\", fontsize=16)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "np.save(os.path.join(save_dir, \"D_losses.npy\"), np.array(D_losses))\n",
    "np.save(os.path.join(save_dir, \"G_losses.npy\"), np.array(G_losses))\n",
    "torch.save(G.state_dict(), os.path.join(save_dir, \"generator.pth\"))\n",
    "torch.save(D.state_dict(), os.path.join(save_dir, \"discriminator.pth\"))\n",
    "\n",
    "print(\"training done and models/losses saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62be1d1c-5c81-4437-969b-106d6e16b3cf",
   "metadata": {},
   "source": [
    "### One image for EACH CLASS with name shown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894ee372-ecb4-440b-9ed0-730eb1e2df5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Load the trained Generator\n",
    "G = Generator128().to(DEVICE)\n",
    "G.load_state_dict(torch.load(os.path.join(save_dir, \"generator.pth\")))\n",
    "G.eval()\n",
    "\n",
    "# Load class names\n",
    "cub_classes = []\n",
    "with open(os.path.join(cub_root, \"classes.txt\")) as f:\n",
    "    for line in f:\n",
    "        cub_classes.append(line.strip().split(\" \", 1)[1])\n",
    "\n",
    "# Generate one bird per class\n",
    "def generate_bird_for_each_class():\n",
    "    z = torch.randn(y_dim, z_dim, device=DEVICE)  # 200 random noise vectors\n",
    "    labels = torch.arange(y_dim, device=DEVICE)   # Labels 0 to 199\n",
    "    labels_onehot = make_one_hot(labels, y_dim)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        fake_imgs = G(z, labels_onehot)\n",
    "\n",
    "    fake_imgs = (fake_imgs + 1) / 2  # Denormalize\n",
    "    fake_imgs = fake_imgs.detach().cpu()\n",
    "\n",
    "    # Plotting\n",
    "    ncols = 10\n",
    "    nrows = math.ceil(y_dim / ncols)\n",
    "    fig, axes = plt.subplots(nrows, ncols, figsize=(ncols * 2, nrows * 2))\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for img, label, ax in zip(fake_imgs, labels, axes):\n",
    "        img = img.permute(1, 2, 0)\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(cub_classes[label], fontsize=5)\n",
    "        ax.axis('off')\n",
    "\n",
    "    for ax in axes[len(fake_imgs):]:\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Call the function\n",
    "generate_bird_for_each_class()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f6223b-4c47-4d02-8f03-e4d5de831f39",
   "metadata": {},
   "source": [
    "### All CUB-200 Bird Class Names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135b9070-0e19-4ce4-8089-ec2ace751bcc",
   "metadata": {},
   "source": [
    "1. Black_Footed_Albatross  \n",
    "2. Laysan_Albatross  \n",
    "3. Sooty_Albatross  \n",
    "4. Groove_Billed_Ani  \n",
    "5. Crested_Auklet  \n",
    "6. Least_Auklet  \n",
    "7. Parakeet_Auklet  \n",
    "8. Rhinoceros_Auklet  \n",
    "9. Brewer_Blackbird  \n",
    "10. Red_Winged_Blackbird  \n",
    "11. Rusty_Blackbird  \n",
    "12. Yellow_Headed_Blackbird  \n",
    "13. Bobolink  \n",
    "14. Indigo_Bunting  \n",
    "15. Lazuli_Bunting  \n",
    "16. Painted_Bunting  \n",
    "17. Cardinal  \n",
    "18. Spotted_Catbird  \n",
    "19. Gray_Catbird  \n",
    "20. Yellow_Breasted_Chat  \n",
    "21. Eastern_Towhee  \n",
    "22. Chuck_Will_Widow  \n",
    "23. Brandt_Cormorant  \n",
    "24. Red_Faced_Cormorant  \n",
    "25. Pelagic_Cormorant  \n",
    "26. Bronzed_Cowbird  \n",
    "27. Shiny_Cowbird  \n",
    "28. Brown_Creeper  \n",
    "29. American_Crow  \n",
    "30. Fish_Crow  \n",
    "31. Black_Billed_Cuckoo  \n",
    "32. Mangrove_Cuckoo  \n",
    "33. Yellow_Billed_Cuckoo  \n",
    "34. Gray_Crowned_Rosy_Finch  \n",
    "35. Purple_Finch  \n",
    "36. Northern_Flicker  \n",
    "37. Acadian_Flycatcher  \n",
    "38. Great_Crested_Flycatcher  \n",
    "39. Least_Flycatcher  \n",
    "40. Olive_Sided_Flycatcher  \n",
    "41. Scissor_Tailed_Flycatcher  \n",
    "42. Vermilion_Flycatcher  \n",
    "43. Yellow_Bellied_Flycatcher  \n",
    "44. Frigatebird  \n",
    "45. Northern_Fulmar  \n",
    "46. Gadwall  \n",
    "47. American_Goldfinch  \n",
    "48. European_Goldfinch  \n",
    "49. Boat_Tailed_Grackle  \n",
    "50. Eared_Grebe  \n",
    "51. Horned_Grebe  \n",
    "52. Pied_Billed_Grebe  \n",
    "53. Western_Grebe  \n",
    "54. Blue_Grosbeak  \n",
    "55. Evening_Grosbeak  \n",
    "56. Pine_Grosbeak  \n",
    "57. Rose_Breasted_Grosbeak  \n",
    "58. Pigeon_Guillemot  \n",
    "59. California_Gull  \n",
    "60. Glaucous_Winged_Gull  \n",
    "61. Heermann_Gull  \n",
    "62. Herring_Gull  \n",
    "63. Ivory_Gull  \n",
    "64. Ring_Billed_Gull  \n",
    "65. Slaty_Backed_Gull  \n",
    "66. Western_Gull  \n",
    "67. Anna_Hummingbird  \n",
    "68. Ruby_Throated_Hummingbird  \n",
    "69. Rufous_Hummingbird  \n",
    "70. Green_Violetear  \n",
    "71. Long_Tailed_Jaeger  \n",
    "72. Pomarine_Jaeger  \n",
    "73. Blue_Jay  \n",
    "74. Florida_Jay  \n",
    "75. Green_Jay  \n",
    "76. Dark_Eyed_Junco  \n",
    "77. Northern_Jacana  \n",
    "78. Green_Kingfisher  \n",
    "79. Pied_Kingfisher  \n",
    "80. Ringed_Kingfisher  \n",
    "81. Belted_Kingfisher  \n",
    "82. White_Breasted_Kingfisher  \n",
    "83. Red_Legged_Kittiwake  \n",
    "84. Horned_Lark  \n",
    "85. Pacific_Loon  \n",
    "86. Mallard  \n",
    "87. Western_Meadowlark  \n",
    "88. Hooded_Merganser  \n",
    "89. Red_Breasted_Merganser  \n",
    "90. Mockingbird  \n",
    "91. Nighthawk  \n",
    "92. Clark_Nutcracker  \n",
    "93. White_Breasted_Nuthatch  \n",
    "94. Baltimore_Oriole  \n",
    "95. Hooded_Oriole  \n",
    "96. Orchard_Oriole  \n",
    "97. Scott_Oriole  \n",
    "98. Ovenbird  \n",
    "99. Brown_Pelican  \n",
    "100. White_Pelican  \n",
    "101. Western_Wood_Pewee  \n",
    "102. Sayornis  \n",
    "103. American_Pipit  \n",
    "104. Whip_Poor_Will  \n",
    "105. Horned_Puffin  \n",
    "106. Common_Raven  \n",
    "107. White_Necked_Raven  \n",
    "108. American_Redstart  \n",
    "109. Geococcyx  \n",
    "110. Loggerhead_Shrike  \n",
    "111. Great_Grey_Shrike  \n",
    "112. Baird_Sparrow  \n",
    "113. Black_Throated_Sparrow  \n",
    "114. Brewer_Sparrow  \n",
    "115. Chipping_Sparrow  \n",
    "116. Clay_Colored_Sparrow  \n",
    "117. House_Sparrow  \n",
    "118. Field_Sparrow  \n",
    "119. Fox_Sparrow  \n",
    "120. Grasshopper_Sparrow  \n",
    "121. Harris_Sparrow  \n",
    "122. Henslow_Sparrow  \n",
    "123. Le_Conte_Sparrow  \n",
    "124. Lincoln_Sparrow  \n",
    "125. Nelson_Sharp_Tailed_Sparrow  \n",
    "126. Savannah_Sparrow  \n",
    "127. Seaside_Sparrow  \n",
    "128. Song_Sparrow  \n",
    "129. Tree_Sparrow  \n",
    "130. Vesper_Sparrow  \n",
    "131. White_Crowned_Sparrow  \n",
    "132. White_Throated_Sparrow  \n",
    "133. Cape_Glossy_Starling  \n",
    "134. Bank_Swallow  \n",
    "135. Barn_Swallow  \n",
    "136. Cliff_Swallow  \n",
    "137. Tree_Swallow  \n",
    "138. Scarlet_Tanager  \n",
    "139. Summer_Tanager  \n",
    "140. Artic_Tern  \n",
    "141. Black_Tern  \n",
    "142. Caspian_Tern  \n",
    "143. Common_Tern  \n",
    "144. Elegant_Tern  \n",
    "145. Forsters_Tern  \n",
    "146. Least_Tern  \n",
    "147. Green_Tailed_Towhee  \n",
    "148. Brown_Thrasher  \n",
    "149. Sage_Thrasher  \n",
    "150. Black_Capped_Vireo  \n",
    "151. Blue_Headed_Vireo  \n",
    "152. Philadelphia_Vireo  \n",
    "153. Red_Eyed_Vireo  \n",
    "154. Warbling_Vireo  \n",
    "155. White_Eyed_Vireo  \n",
    "156. Yellow_Throated_Vireo  \n",
    "157. Bay_Breasted_Warbler  \n",
    "158. Black_And_White_Warbler  \n",
    "159. Black_Throated_Blue_Warbler  \n",
    "160. Blue_Winged_Warbler  \n",
    "161. Canada_Warbler  \n",
    "162. Cape_May_Warbler  \n",
    "163. Cerulean_Warbler  \n",
    "164. Chestnut_Sided_Warbler  \n",
    "165. Golden_Winged_Warbler  \n",
    "166. Hooded_Warbler  \n",
    "167. Kentucky_Warbler  \n",
    "168. Magnolia_Warbler  \n",
    "169. Mourning_Warbler  \n",
    "170. Myrtle_Warbler  \n",
    "171. Nashville_Warbler  \n",
    "172. Orange_Crowned_Warbler  \n",
    "173. Palm_Warbler  \n",
    "174. Pine_Warbler  \n",
    "175. Prairie_Warbler  \n",
    "176. Prothonotary_Warbler  \n",
    "177. Swainson_Warbler  \n",
    "178. Tennessee_Warbler  \n",
    "179. Wilson_Warbler  \n",
    "180. Worm_Eating_Warbler  \n",
    "181. Yellow_Warbler  \n",
    "182. Northern_Waterthrush  \n",
    "183. Louisiana_Waterthrush  \n",
    "184. Bohemian_Waxwing  \n",
    "185. Cedar_Waxwing  \n",
    "186. American_Three_Toed_Woodpecker  \n",
    "187. Downy_Woodpecker  \n",
    "188. Hairy_Woodpecker  \n",
    "189. Red_Bellied_Woodpecker  \n",
    "190. Red_Cockaded_Woodpecker  \n",
    "191. Pileated_Woodpecker  \n",
    "192. Red_Headed_Woodpecker  \n",
    "193. White_Breasted_Woodpecker  \n",
    "194. American_Three_Toed_Woodpecker  \n",
    "195. Bewick_Wren  \n",
    "196. Cactus_Wren  \n",
    "197. Carolina_Wren  \n",
    "198. House_Wren  \n",
    "199. Marsh_Wren  \n",
    "200. Rock_Wren"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2735b0cd-5a4f-4d65-bc26-edc5ae75349a",
   "metadata": {},
   "source": [
    "##### We will be entering (target label = (corresponding number - 1)) for the bird we want to generate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce817a6f-18f4-4d68-8ac2-139218d6430c",
   "metadata": {},
   "source": [
    "#### Example : Yellow_Warbler (Label = 181)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccbf483f-9b39-454e-b135-f98781c8c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "# Load trained Generator\n",
    "G = Generator128().to(DEVICE)\n",
    "G.load_state_dict(torch.load(os.path.join(save_dir, \"generator.pth\"), map_location=DEVICE))\n",
    "G.eval()\n",
    "\n",
    "# Load class names\n",
    "class_names = []\n",
    "with open(os.path.join(cub_root, \"classes.txt\")) as f:\n",
    "    class_names = [line.strip().split(\" \", 1)[1] for line in f.readlines()]\n",
    "\n",
    "target_label = 181\n",
    "\n",
    "# 1. Pick a random real image\n",
    "real_indices = [i for i, lbl in enumerate(cub_dataset.labels) if lbl == target_label]\n",
    "random_real_idx = random.choice(real_indices)\n",
    "real_img_path = os.path.join(cub_root, \"images\", cub_dataset.image_paths[random_real_idx])\n",
    "real_img = Image.open(real_img_path).convert(\"RGB\")\n",
    "real_img = transform(real_img)  # Apply same transform as training\n",
    "real_img = (real_img + 1) / 2   # Denormalize to [0,1]\n",
    "real_img = real_img.permute(1, 2, 0).cpu()\n",
    "\n",
    "# 2. Generate a fake image\n",
    "z = torch.randn(1, z_dim, device=DEVICE)\n",
    "label_tensor = torch.tensor([target_label], device=DEVICE)\n",
    "label_onehot = make_one_hot(label_tensor, y_dim)\n",
    "\n",
    "with torch.no_grad():\n",
    "    fake_img = G(z, label_onehot)\n",
    "\n",
    "fake_img = (fake_img + 1) / 2  # Denormalize\n",
    "fake_img = fake_img.squeeze(0).detach().cpu()\n",
    "fake_img = fake_img.permute(1, 2, 0)\n",
    "\n",
    "# Plot Real vs Fake\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "axes[0].imshow(real_img)\n",
    "axes[0].set_title(f\"Real: {class_names[target_label]}\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(fake_img)\n",
    "axes[1].set_title(f\"Fake: {class_names[target_label]}\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.suptitle(f\"Comparison: Real vs Fake - {class_names[target_label]}\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd19786-c0da-4a1c-8887-7bd970298d50",
   "metadata": {},
   "source": [
    "#### Example : Blue_Winged_Warbler (Label = 160)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7f3de3-fa02-4115-9589-5c0b06a980ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "# Load trained Generator\n",
    "G = Generator128().to(DEVICE)\n",
    "G.load_state_dict(torch.load(os.path.join(save_dir, \"generator.pth\"), map_location=DEVICE))\n",
    "G.eval()\n",
    "\n",
    "# Load class names\n",
    "class_names = []\n",
    "with open(os.path.join(cub_root, \"classes.txt\")) as f:\n",
    "    class_names = [line.strip().split(\" \", 1)[1] for line in f.readlines()]\n",
    "\n",
    "target_label = 160\n",
    "\n",
    "# 1. Pick a random real image\n",
    "real_indices = [i for i, lbl in enumerate(cub_dataset.labels) if lbl == target_label]\n",
    "random_real_idx = random.choice(real_indices)\n",
    "real_img_path = os.path.join(cub_root, \"images\", cub_dataset.image_paths[random_real_idx])\n",
    "real_img = Image.open(real_img_path).convert(\"RGB\")\n",
    "real_img = transform(real_img)  # Apply same transform as training\n",
    "real_img = (real_img + 1) / 2   # Denormalize to [0,1]\n",
    "real_img = real_img.permute(1, 2, 0).cpu()\n",
    "\n",
    "# 2. Generate a fake image\n",
    "z = torch.randn(1, z_dim, device=DEVICE)\n",
    "label_tensor = torch.tensor([target_label], device=DEVICE)\n",
    "label_onehot = make_one_hot(label_tensor, y_dim)\n",
    "\n",
    "with torch.no_grad():\n",
    "    fake_img = G(z, label_onehot)\n",
    "\n",
    "fake_img = (fake_img + 1) / 2  # Denormalize\n",
    "fake_img = fake_img.squeeze(0).detach().cpu()\n",
    "fake_img = fake_img.permute(1, 2, 0)\n",
    "\n",
    "# Plot Real vs Fake\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "axes[0].imshow(real_img)\n",
    "axes[0].set_title(f\"Real: {class_names[target_label]}\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(fake_img)\n",
    "axes[1].set_title(f\"Fake: {class_names[target_label]}\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.suptitle(f\"Comparison: Real vs Fake - {class_names[target_label]}\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3531137-095d-4886-93b6-8d66575fce35",
   "metadata": {},
   "source": [
    "#### Example : Black_Throated_Sparrow (Label = 113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ed95e3-8081-4848-ba17-1796f50c3d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "# Load trained Generator\n",
    "G = Generator128().to(DEVICE)\n",
    "G.load_state_dict(torch.load(os.path.join(save_dir, \"generator.pth\"), map_location=DEVICE))\n",
    "G.eval()\n",
    "\n",
    "# Load class names\n",
    "class_names = []\n",
    "with open(os.path.join(cub_root, \"classes.txt\")) as f:\n",
    "    class_names = [line.strip().split(\" \", 1)[1] for line in f.readlines()]\n",
    "\n",
    "target_label = 113\n",
    "\n",
    "# 1. Pick a random real image\n",
    "real_indices = [i for i, lbl in enumerate(cub_dataset.labels) if lbl == target_label]\n",
    "random_real_idx = random.choice(real_indices)\n",
    "real_img_path = os.path.join(cub_root, \"images\", cub_dataset.image_paths[random_real_idx])\n",
    "real_img = Image.open(real_img_path).convert(\"RGB\")\n",
    "real_img = transform(real_img)  # Apply same transform as training\n",
    "real_img = (real_img + 1) / 2   # Denormalize to [0,1]\n",
    "real_img = real_img.permute(1, 2, 0).cpu()\n",
    "\n",
    "# 2. Generate a fake image\n",
    "z = torch.randn(1, z_dim, device=DEVICE)\n",
    "label_tensor = torch.tensor([target_label], device=DEVICE)\n",
    "label_onehot = make_one_hot(label_tensor, y_dim)\n",
    "\n",
    "with torch.no_grad():\n",
    "    fake_img = G(z, label_onehot)\n",
    "\n",
    "fake_img = (fake_img + 1) / 2  # Denormalize\n",
    "fake_img = fake_img.squeeze(0).detach().cpu()\n",
    "fake_img = fake_img.permute(1, 2, 0)\n",
    "\n",
    "# Plot Real vs Fake\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "axes[0].imshow(real_img)\n",
    "axes[0].set_title(f\"Real: {class_names[target_label]}\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(fake_img)\n",
    "axes[1].set_title(f\"Fake: {class_names[target_label]}\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.suptitle(f\"Comparison: Real vs Fake - {class_names[target_label]}\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6feff9-c862-4a35-a575-f86a12e27348",
   "metadata": {},
   "source": [
    "#### Example : Florida_Jay (Label = 74)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e4bfd0-5ac3-42fd-a371-3db10cb1302d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "# Load trained Generator\n",
    "G = Generator128().to(DEVICE)\n",
    "G.load_state_dict(torch.load(os.path.join(save_dir, \"generator.pth\"), map_location=DEVICE))\n",
    "G.eval()\n",
    "\n",
    "# Load class names\n",
    "class_names = []\n",
    "with open(os.path.join(cub_root, \"classes.txt\")) as f:\n",
    "    class_names = [line.strip().split(\" \", 1)[1] for line in f.readlines()]\n",
    "\n",
    "target_label = 73\n",
    "\n",
    "# 1. Pick a random real image\n",
    "real_indices = [i for i, lbl in enumerate(cub_dataset.labels) if lbl == target_label]\n",
    "random_real_idx = random.choice(real_indices)\n",
    "real_img_path = os.path.join(cub_root, \"images\", cub_dataset.image_paths[random_real_idx])\n",
    "real_img = Image.open(real_img_path).convert(\"RGB\")\n",
    "real_img = transform(real_img)  # Apply same transform as training\n",
    "real_img = (real_img + 1) / 2   # Denormalize to [0,1]\n",
    "real_img = real_img.permute(1, 2, 0).cpu()\n",
    "\n",
    "# 2. Generate a fake image\n",
    "z = torch.randn(1, z_dim, device=DEVICE)\n",
    "label_tensor = torch.tensor([target_label], device=DEVICE)\n",
    "label_onehot = make_one_hot(label_tensor, y_dim)\n",
    "\n",
    "with torch.no_grad():\n",
    "    fake_img = G(z, label_onehot)\n",
    "\n",
    "fake_img = (fake_img + 1) / 2  # Denormalize\n",
    "fake_img = fake_img.squeeze(0).detach().cpu()\n",
    "fake_img = fake_img.permute(1, 2, 0)\n",
    "\n",
    "# Plot Real vs Fake\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "axes[0].imshow(real_img)\n",
    "axes[0].set_title(f\"Real: {class_names[target_label]}\")\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(fake_img)\n",
    "axes[1].set_title(f\"Fake: {class_names[target_label]}\")\n",
    "axes[1].axis('off')\n",
    "\n",
    "plt.suptitle(f\"Comparison: Real vs Fake - {class_names[target_label]}\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd454c5-6d6e-4d5f-92b0-8965cd92d79d",
   "metadata": {},
   "source": [
    "### Generating using Class Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d855c9-6704-41ae-8d2e-0d976f95eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import difflib\n",
    "\n",
    "name_to_label = {}\n",
    "class_names = []\n",
    "with open(os.path.join(cub_root, \"classes.txt\")) as f:\n",
    "    for idx, line in enumerate(f):\n",
    "        name = line.strip().split(\" \", 1)[1]\n",
    "        norm = name.lower().replace(\"_\", \" \")\n",
    "        class_names.append(name)\n",
    "        name_to_label[norm] = idx\n",
    "\n",
    "def prompt_to_label(prompt):\n",
    "    \"\"\"Fuzzy‐match your text prompt to the closest CUB-200 class.\"\"\"\n",
    "    key = prompt.lower().strip()\n",
    "    key = key.replace(\"_\", \" \")\n",
    "    if key in name_to_label:\n",
    "        return name_to_label[key]\n",
    "    # otherwise use difflib to pick the closest match\n",
    "    guesses = difflib.get_close_matches(key, name_to_label.keys(), n=1, cutoff=0.6)\n",
    "    if not guesses:\n",
    "        raise ValueError(f\"No close match found for '{prompt}'\")\n",
    "    return name_to_label[guesses[0]]\n",
    "\n",
    "def generate_from_text(prompt):\n",
    "    \"\"\"Generate a single fake bird given its English name.\"\"\"\n",
    "    label = prompt_to_label(prompt)\n",
    "    z = torch.randn(1, z_dim, device=DEVICE)\n",
    "    oh = make_one_hot(torch.tensor([label], device=DEVICE), y_dim)\n",
    "    with torch.no_grad():\n",
    "        img = G(z, oh)\n",
    "    img = (img + 1)/2\n",
    "    img = img.squeeze(0).cpu().permute(1,2,0)\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.imshow(img)\n",
    "    plt.title(f\"{class_names[label]} ({label})\", fontsize=10)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbf38ce-1cf9-4b75-9bcc-98121c2c990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_from_text(\"Yellow Warbler\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56ca26a-c8d6-41dc-9e1c-6669f3c8c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_from_text(\"blue jay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9864429-84cb-4267-9448-a6e2958dbdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_from_text(\"American Three Toed Woodpecker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594ca2f1-0b4b-4dd5-8b25-e36ffe41ff94",
   "metadata": {},
   "source": [
    "### FID / IS for CUB-200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b6db16-00b8-4cd6-9295-e1323e64af81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "from torchmetrics.image.inception import InceptionScore\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "num_gen = 1000\n",
    "batch_eval = 64\n",
    "\n",
    "\n",
    "G = Generator128().to(DEVICE)\n",
    "G.load_state_dict(torch.load(os.path.join(save_dir, \"generator.pth\"), map_location=DEVICE))\n",
    "G.eval()\n",
    "\n",
    "def make_one_hot(labels, num_classes):\n",
    "    return torch.zeros(labels.size(0), num_classes, device=labels.device).scatter_(1, labels.unsqueeze(1), 1)\n",
    "\n",
    "\n",
    "fake_imgs_list = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(0, num_gen, batch_eval):\n",
    "        z = torch.randn(batch_eval, z_dim, device=DEVICE)\n",
    "        labels = torch.randint(0, y_dim, (batch_eval,), device=DEVICE)\n",
    "        one_hot_labels = make_one_hot(labels, y_dim)\n",
    "        fake_imgs = G(z, one_hot_labels).cpu()\n",
    "        fake_imgs = (fake_imgs + 1) / 2  # [-1,1] to [0,1]\n",
    "        fake_imgs = (fake_imgs * 255).clamp(0, 255).to(torch.uint8)\n",
    "        fake_imgs_list.append(fake_imgs)\n",
    "\n",
    "fake_imgs_uint8 = torch.cat(fake_imgs_list, dim=0)\n",
    "\n",
    "\n",
    "real_imgs_list = []\n",
    "real_loader = DataLoader(cub_dataset, batch_size=batch_eval, shuffle=True, drop_last=True, num_workers=0)\n",
    "\n",
    "needed = num_gen\n",
    "for real_imgs, _ in real_loader:\n",
    "    if needed <= 0:\n",
    "        break\n",
    "    real = real_imgs[:needed]\n",
    "    real = (real * 0.5 + 0.5)  # [-1,1] to [0,1]\n",
    "    real = (real * 255).clamp(0, 255).to(torch.uint8)\n",
    "    real_imgs_list.append(real)\n",
    "    needed -= real.shape[0]\n",
    "\n",
    "real_imgs_uint8 = torch.cat(real_imgs_list, dim=0)\n",
    "\n",
    "fid = FrechetInceptionDistance(feature=64, normalize=True).to('cpu')\n",
    "for i in range(0, num_gen, batch_eval):\n",
    "    fid.update(real_imgs_uint8[i:i+batch_eval], real=True)\n",
    "    fid.update(fake_imgs_uint8[i:i+batch_eval], real=False)\n",
    "fid_score = fid.compute().item()\n",
    "\n",
    "is_scorer = InceptionScore(normalize=True).to('cpu')\n",
    "for i in range(0, num_gen, batch_eval):\n",
    "    is_scorer.update(fake_imgs_uint8[i:i+batch_eval])\n",
    "is_score, is_std = is_scorer.compute()\n",
    "\n",
    "print(f\"\\nFID (real vs generated): {fid_score:.2f}\")\n",
    "print(f\"Inception Score: {is_score:.2f} ± {is_std:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1d3aff-6c02-42cd-b5a3-48e9fc615418",
   "metadata": {},
   "source": [
    "### Loss Plot for CUB-200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3c3e90e-d336-49f8-a200-6fd1bac327a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "def moving_average(x, window_size=200):\n",
    "    return np.convolve(x, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "# Path to saved losses\n",
    "save_dir = \"./outputs/CUB200\"\n",
    "D_losses = np.load(os.path.join(save_dir, \"D_losses.npy\"))\n",
    "G_losses = np.load(os.path.join(save_dir, \"G_losses.npy\"))\n",
    "\n",
    "# Plot smoothed loss curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(moving_average(D_losses), label=\"Discriminator Loss\", color=\"tab:blue\", alpha=0.9)\n",
    "plt.plot(moving_average(G_losses), label=\"Generator Loss\", color=\"tab:orange\", alpha=0.9)\n",
    "plt.xlabel(\"Training Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"CUB-200 cGAN Training Loss Curves (Smoothed)\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(save_dir, \"cub200_cgan_loss_curve.png\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9440ddb-6f7f-4a9c-8ecc-4f31b1b72137",
   "metadata": {},
   "source": [
    "# Completed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
