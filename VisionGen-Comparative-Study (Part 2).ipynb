{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16cf0f0f-da54-41f0-a9f1-1f76dbb5f13f",
   "metadata": {},
   "source": [
    "# VisionGen-Comparative-Study (Part 2)\n",
    "\n",
    "_A Comparative Analysis of CGANs and Diffusion Models for Conditional Image Synthesis_\n",
    "\n",
    "---\n",
    "\n",
    "## Motivation\n",
    "\n",
    "Generative models are transforming the field of computer vision, enabling AI systems to generate realistic images from simple inputs like class labels or text prompts. This project investigates:\n",
    "- How do CGANs and Diffusion Models perform on classic and challenging datasets?\n",
    "- What are the differences in sample quality, training stability, and controllability?\n",
    "- Which model is more suitable for high-fidelity, class-conditional image synthesis?\n",
    "\n",
    "## Approach\n",
    "\n",
    "- **CGANs:**  \n",
    "  Implemented and trained on MNIST (digits), Oxford-102 Flowers, and CUB-200 Birds datasets, using fully connected and convolutional architectures.\n",
    "\n",
    "- **Diffusion Models:**  \n",
    "  Integrated modern diffusion pipelines (e.g., DDPM, Stable Diffusion) using open-source libraries. Evaluated on the same datasets for direct comparison.\n",
    "\n",
    "- **Comparative Analysis:**  \n",
    "  Results evaluated both quantitatively (FID, Inception Score) and qualitatively (side-by-side image grids, training dynamics).\n",
    "\n",
    "## Key Features\n",
    "\n",
    "- End-to-end PyTorch and HuggingFace-based implementation\n",
    "- Clean code, modular design, and ready-to-run Jupyter notebooks\n",
    "- Direct, apples-to-apples comparison of CGANs and Diffusion Models\n",
    "- Sample outputs, evaluation metrics, and visualizations included\n",
    "\n",
    "---\n",
    "\n",
    "## Author\n",
    "\n",
    "**Ayushman Mishra**  \n",
    "[GitHub: frMishR](https://github.com/frMishR)\n",
    "\n",
    "---\n",
    "\n",
    "## Research Inspiration\n",
    "\n",
    "This project draws inspiration from foundational work in generative modeling, especially:\n",
    "\n",
    "- **Conditional Generative Adversarial Nets (Mirza & Osindero, 2014):**  \n",
    "  [arXiv:1411.1784](https://arxiv.org/abs/1411.1784)  \n",
    "  The original CGAN paper provided the conceptual and technical basis for the CGAN implementations.\n",
    "\n",
    "Recent advances in diffusion models and open-source research also guided the design of experiments and code.\n",
    "\n",
    "---\n",
    "\n",
    "## Project Origin\n",
    "\n",
    "This project originated as a group submission for:\n",
    "\n",
    "- **Course:** EEE 598: Generative AI – Theory and Practice  \n",
    "- **Professor:** Dr. Lalitha Sankar (Arizona State University)  \n",
    "- **Semester:** Spring 2025\n",
    "\n",
    "### Original Contributors\n",
    "\n",
    "- **Ayushman Mishra** (Solo Upgrade & Comparative Study, 2025)\n",
    "- **Snavya Sai Munti Mudugu Badri Prasad** [GitHub: snavya0309](https://github.com/snavya0309)\n",
    "- **Sushma Niresh** [GitHub: SushmaNiresh](https://github.com/SushmaNiresh)\n",
    "\n",
    "> *Note: This repository represents a major solo upgrade and extension by Ayushman Mishra. All diffusion modeling, comparative analysis, and new documentation were developed independently by Ayushman Mishra (July–September 2025). The original CGAN implementation and dataset preparation were developed collaboratively by the above group.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f903acd7-fb24-490f-8adc-60582db46473",
   "metadata": {},
   "source": [
    "# VisionGen – Class-Conditional Diffusion (DDPM) on Multi-Class Datasets\n",
    "\n",
    "> **Notebook Focus**: This notebook implements a **class-conditional Diffusion model (DDPM)** using a UNet backbone with `num_class_embeds` for label conditioning. We use the **same datasets** as Part 1 (e.g., **MNIST**, **Oxford-102 Flowers**, **CUB-200 Birds**) to enable an **apples-to-apples comparison** with cDCGAN.\n",
    "\n",
    "---\n",
    "\n",
    "## Objective\n",
    "\n",
    "Move beyond adversarial training (GANs) to **score-based diffusion**.  \n",
    "Diffusion models learn to **denoise images** from pure noise through a sequence of timesteps, offering **training stability** and **high-fidelity samples**. Here we make the model **class-conditional**, so generation can be guided by labels (e.g., “rose,” “eagle,” “digit 3”).\n",
    "\n",
    "---\n",
    "\n",
    "## Key Concepts Covered\n",
    "\n",
    "- **Forward/Reverse Diffusion**:  \n",
    "  - *Forward*: add Gaussian noise to images over T steps.  \n",
    "  - *Reverse*: learn to **predict the noise** and iteratively denoise.\n",
    "- **DDPM Training Objective**: Predict ε (noise) with MSE; scheduler defines β-schedule and timesteps.\n",
    "- **Label Conditioning**: UNet receives **class embeddings**; sampling is controlled via class IDs.\n",
    "- **Schedulers**: Linear or cosine (“`squaredcos_cap_v2`”) β-schedules; inference uses the learned reverse process.\n",
    "- **Sampling**: Iterative denoising loop creates class-specific images from pure noise.\n",
    "- **(Optional)** **FID/IS**: Quantitative comparison against validation sets; supports **CGAN vs. DDPM** analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## Notebook Structure\n",
    "\n",
    "1. **Imports & Setup** – `torch`, `diffusers` (UNet + `DDPMScheduler`), device config, seeds  \n",
    "2. **Config & Hyperparameters** – `image_size`, `batch_size`, `epochs`, β-schedule, `num_train_timesteps`  \n",
    "3. **Dataset Handling (same splits as Part 1)** – `ImageFolder` for Flowers/CUB, MNIST helper, transforms & normalization  \n",
    "4. **Model Definition** – `UNet2DModel` with `num_class_embeds = K` and channels {1|3}  \n",
    "5. **Training Loop** – random timestep per sample → add noise → predict ε → MSE loss → optimizer step  \n",
    "6. **Sampling Routine** – iterative denoise with chosen labels; save class-grids each epoch  \n",
    "7. **(Optional) Metrics** – generate N images per class; compute **FID** via `torch-fidelity`  \n",
    "8. **Checkpointing & Reload** – save `UNet` weights + scheduler; quick reload for inference\n",
    "\n",
    "---\n",
    "\n",
    "## Why this matters\n",
    "\n",
    "- **Stability & Fidelity**: Diffusion training is typically more stable than GANs and often yields **cleaner textures** and **fewer mode-collapse artifacts**.  \n",
    "- **Controlled Generation**: Class conditioning enables **targeted image synthesis** for multi-class datasets.  \n",
    "- **Real-world Signal**: Shows you can implement modern **score-based generative modeling**, not just adversarial setups.\n",
    "\n",
    "---\n",
    "\n",
    "## Comparative Angle (Part 1 vs Part 2)\n",
    "\n",
    "| Aspect | cDCGAN (Part 1) | DDPM (Part 2) |\n",
    "|---|---|---|\n",
    "| Training | Adversarial (G vs D) | Denoising (predict ε) |\n",
    "| Stability | Trickier (GAN dynamics) | Generally stable |\n",
    "| Speed | Often faster per epoch | Slower (many timesteps) |\n",
    "| Fidelity | Good, may artifact | Often very clean |\n",
    "| Conditioning | Label concat/embeds | `num_class_embeds` in UNet |\n",
    "| Metrics | FID/IS | FID/IS |\n",
    "\n",
    "> **Deliverable**: A **side-by-side table** of FID and sample grids for the same classes/datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## Reproducibility & Usage Notes\n",
    "\n",
    "- Fix seeds; log losses and sample grids at consistent epochs.  \n",
    "- Keep **image size** and **train splits** identical to Part 1 for fair comparison.  \n",
    "- For laptop GPUs: start with **64×64**, lower batch size, fewer epochs; scale up once the pipeline is verified. \n",
    "\n",
    "---\n",
    "\n",
    "## Outcome\n",
    "\n",
    "This notebook provides a **clean Diffusion baseline** to compare with your **cDCGAN** results—enabling a credible **Comparative Study** across **architecture families**, **datasets**, and **metrics**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbe3675-e3ed-4454-a320-060fe8afd77c",
   "metadata": {},
   "source": [
    "### Setting up GPU & Environment Check\n",
    "\n",
    "Sanity-check your setup before training:\n",
    "- Prints **Python**, **PyTorch**, **torchvision**, **diffusers**, **transformers** versions.\n",
    "- Detects **CUDA** and reports GPU name + VRAM.\n",
    "- Creates a `device` you’ll reuse later (`cuda` or `cpu`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "899dd458-b865-4c60-bb0f-efdbe42c4c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.10.0 (Windows)\n",
      "Torch: 2.5.1+cu121\n",
      "torchvision: 0.20.1+cu121\n",
      "diffusers: 0.35.1\n",
      "transformers: not installed\n",
      "Device: CUDA:0 — NVIDIA GeForce RTX 4060 Laptop GPU (CC 8.9, 8.0 GB VRAM)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys, platform\n",
    "import torch\n",
    "\n",
    "def _ver(mod):\n",
    "    try:\n",
    "        return mod.__version__\n",
    "    except Exception:\n",
    "        return \"N/A\"\n",
    "\n",
    "print(f\"Python: {sys.version.split()[0]} ({platform.system()})\")\n",
    "print(f\"Torch: {torch.__version__}\")\n",
    "\n",
    "# Optional libs\n",
    "try:\n",
    "    import torchvision\n",
    "    print(\"torchvision:\", _ver(torchvision))\n",
    "except Exception:\n",
    "    print(\"torchvision: not installed\")\n",
    "\n",
    "try:\n",
    "    import diffusers\n",
    "    print(\"diffusers:\", _ver(diffusers))\n",
    "except Exception:\n",
    "    print(\"diffusers: not installed\")\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    print(\"transformers:\", _ver(transformers))\n",
    "except Exception:\n",
    "    print(\"transformers: not installed\")\n",
    "\n",
    "# Device info\n",
    "if torch.cuda.is_available():\n",
    "    dev_id = torch.cuda.current_device()\n",
    "    props = torch.cuda.get_device_properties(dev_id)\n",
    "    cap = torch.cuda.get_device_capability(dev_id)\n",
    "    vram_gb = props.total_memory / (1024**3)\n",
    "    print(f\"Device: CUDA:{dev_id} — {props.name} (CC {cap[0]}.{cap[1]}, {vram_gb:.1f} GB VRAM)\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Device: CPU (CUDA not available)\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa90e027-9372-44cd-bfb1-93cc20c7fbe1",
   "metadata": {},
   "source": [
    "### Utilities\n",
    "\n",
    "We define helper functions for **reproducibility** and **visualization**:\n",
    "\n",
    "- `set_seed(seed)`: Fixes randomness across `random`, `numpy`, and `torch` so runs are deterministic and comparable.  \n",
    "- `show_grid(tensor, nrow, title)`: Nicely display a batch of images in a grid, converting them back from `[-1,1]` to `[0,1]` if needed.\n",
    "\n",
    "These are lightweight utilities used throughout training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bafd2304-b914-4207-a17c-52e41d15583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    print(f\"Seed fixed at {seed}\")\n",
    "\n",
    "def show_grid(tensor, nrow=8, title=None):\n",
    "    \"\"\"Display a grid of images (expects [B,C,H,W])\"\"\"\n",
    "    t = tensor.detach().cpu()\n",
    "    if t.min() < 0:   # convert from [-1,1] to [0,1]\n",
    "        t = (t + 1) / 2\n",
    "    grid = make_grid(t, nrow=nrow, pad_value=0.5)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.axis(\"off\")\n",
    "    if title:\n",
    "        plt.title(title)\n",
    "    plt.imshow(grid.permute(1,2,0))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199f938c-741b-4657-a816-f36add0e17ac",
   "metadata": {},
   "source": [
    "### Configuration (what this code does)\n",
    "\n",
    "We centralize all hyperparameters into a **Config dataclass** so that training can be easily adjusted.  \n",
    "\n",
    "Key fields:  \n",
    "- **dataset**: choose from `'mnist'`, `'flowers'`, or `'cub'`.  \n",
    "- **image_size**: resize all images (64 is laptop-friendly; 128+ for higher fidelity).  \n",
    "- **batch_size / epochs / lr**: standard training settings.  \n",
    "- **diffusion params**:  \n",
    "  - `num_train_timesteps` → number of forward diffusion steps (default 1000).  \n",
    "  - `beta_schedule` → schedule for variance growth (`linear`, `scaled_linear`, or `squaredcos_cap_v2`).  \n",
    "  - `prediction_type` → what the model predicts (`epsilon` = noise).  \n",
    "- **logging / sampling**: how often to log loss and save sample grids.  \n",
    "- **checkpointing**: output directory and frequency of saving weights.  \n",
    "\n",
    "This ensures reproducibility and makes it easy to rerun with different datasets or scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "06dd0ae9-154b-432b-8c7f-a6c7999442ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    # Dataset options\n",
    "    dataset: str = \"Oxford102\"      # 'mnist' | 'oxford102' | 'cub'\n",
    "    data_root: str = \"data\"         # expects structured subfolders\n",
    "\n",
    "    # Image & training\n",
    "    image_size: int = 64\n",
    "    channels: int = 3               # 1 for MNIST, 3 otherwise\n",
    "    batch_size: int = 64\n",
    "    num_workers: int = 4\n",
    "    epochs: int = 50\n",
    "    lr: float = 2e-4\n",
    "    betas: tuple = (0.9, 0.999)\n",
    "\n",
    "    # Diffusion hyperparameters\n",
    "    num_train_timesteps: int = 1000\n",
    "    beta_schedule: str = \"linear\"       # or 'scaled_linear', 'squaredcos_cap_v2'\n",
    "    prediction_type: str = \"epsilon\"    # predict noise\n",
    "\n",
    "    # Logging / sampling\n",
    "    log_every: int = 100\n",
    "    sample_every: int = 1               # epochs\n",
    "    n_sample_per_class: int = 8\n",
    "\n",
    "    # Checkpointing\n",
    "    out_dir: str = \"outputs/diffusion\"\n",
    "    ckpt_every: int = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07f1026-f21a-4fe5-bdf2-d9c41d6058c1",
   "metadata": {},
   "source": [
    "### Datasets & Dataloaders (Oxford102 + CUB only)\n",
    "\n",
    "We prepare datasets for the diffusion model:\n",
    "\n",
    "- **Oxford-102 Flowers (`oxford102`)**: Expects `data/oxford102/train` and `data/oxford102/val`.  \n",
    "- **CUB-200 Birds (`cub`)**: Expects `data/cub/train` and `data/cub/val`.  \n",
    "\n",
    "All images are resized to `cfg.image_size`, converted to tensor, and normalized to `[-1,1]`.  \n",
    "We return `train_loader`, `val_loader`, and `num_classes` (102 or 200)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa675532-0d07-4375-8801-62178020f5f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
